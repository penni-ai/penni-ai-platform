# Firestore Schema: Search Pipeline

## Overview
The creator search pipeline persists intermediate state and final outputs inside the `search_pipeline_runs` collection. Each pipeline execution writes exactly four documents (one status document plus three stage documents) so that clients can subscribe to progress, stages can reuse prior results, and operators can debug or replay failures. All documents carry a `ttl` timestamp set to one hour after creation so that the cleanup job can prune them automatically.

## Collection: `search_pipeline_runs`

### Document Types
There are two logical document types stored inside the same collection:

1. **Pipeline status documents** – track overall pipeline progress (document ID: `{pipeline_id}`)
2. **Stage documents** – record detailed data for each stage (document ID: `{pipeline_id}_{STAGE_NAME}`)

---

### 1. Pipeline Status Document

**Document ID:** `{pipeline_id}`

**Purpose:** capture pipeline-wide health, progress, and errors so UI clients have a single place to watch.

**Schema:**
```typescript
{
  pipeline_id: string;                     // UUID generated by the orchestrator
  userId: string;                          // UID of the authenticated caller
  status: "running" | "completed" | "error";   // Current pipeline state
  current_stage: string | null;             // One of SEARCH, BRIGHTDATA, LLM_FIT
  completed_stages: string[];               // Ordered list of finished stages
  overall_progress: number;                // Percentage (0–100)
  start_time: Timestamp;                   // Pipeline start
  end_time: Timestamp | null;              // Completion or error time
  error_message: string | null;            // Populated when status === "error"
  created_at: Timestamp;                   // Server timestamp when first written
  updated_at: Timestamp;                   // Server timestamp on every write
  ttl: Timestamp;                          // Expiration (created_at + 1 hour)
}
```

**Example:**
```json
{
  "pipeline_id": "1699564800a1b2c3d4e5f6",
  "userId": "user_123",
  "status": "running",
  "current_stage": "BRIGHTDATA",
  "completed_stages": ["SEARCH"],
  "overall_progress": 66,
  "start_time": "2024-11-09T10:00:00Z",
  "end_time": null,
  "error_message": null,
  "created_at": "2024-11-09T10:00:00Z",
  "updated_at": "2024-11-09T10:05:00Z",
  "ttl": "2024-11-09T11:00:00Z"
}
```

**Access pattern:**
- Writes happen exclusively via `create_pipeline_status`, `update_pipeline_status`, `complete_pipeline_status`, and `error_pipeline_status` in `stage_utils.py` (server-side Admin SDK).
- Reads come from authenticated clients that poll progress and from Cloud Functions needing orchestration context.

---

### 2. Stage Document

**Document ID:** `{pipeline_id}_{STAGE_NAME}` (e.g., `1699564800a1b2c3d4e5f6_SEARCH`).

**Purpose:** store serialized `CreatorProfile` results, debug payloads, metadata, and errors for each pipeline stage.

**Schema:**
```typescript
{
  pipeline_id: string;                     // Parent pipeline identifier
  userId: string;                          // UID inherited from the orchestrator request
  stage: "SEARCH" | "BRIGHTDATA" | "LLM_FIT";
  status: "completed" | "error";
  profiles: CreatorProfile[];              // Serialized dataclass objects (56+ fields)
  debug: Record<string, any>;              // Stage-specific debug details
  metadata: Record<string, any>;           // IO metadata, stats, durations, etc.
  error_message: string | null;            // Present when status === "error"
  created_at: Timestamp;
  updated_at: Timestamp;
  ttl: Timestamp;                          // Expiration (created_at + 1 hour)
}
```

**Example:**
```json
{
  "pipeline_id": "1699564800a1b2c3d4e5f6",
  "userId": "user_123",
  "stage": "SEARCH",
  "status": "completed",
  "profiles": [
    {
      "lance_db_id": "abc123",
      "account": "beauty_influencer",
      "profile_url": "https://instagram.com/beauty_influencer",
      "followers": 50000,
      "engagement_rate": 3.5
      // ...additional CreatorProfile fields
    }
  ],
  "debug": {
    "search_method": "hybrid",
    "query": "beauty influencers",
    "result_count": 20
  },
  "metadata": {
    "io": {
      "inputs": [],
      "outputs": [
        {"lance_db_id": "abc123", "account": "beauty_influencer"}
      ],
      "meta": {}
    }
  },
  "error_message": null,
  "created_at": "2024-11-09T10:00:00Z",
  "updated_at": "2024-11-09T10:00:00Z",
  "ttl": "2024-11-09T11:00:00Z"
}
```

**Access pattern:**
- Writes via `save_stage_document` and `record_stage_failure` from Cloud Functions only.
- Reads performed by downstream stages (`read_stage_document`, `read_stage_profiles`) and optionally by authenticated clients to fetch final results.

---

## Stage Execution Flow
1. `search_pipeline_orchestrator` generates a `pipeline_id` and creates the pipeline status document.
2. `search_stage` executes first, persists `{pipeline_id}_SEARCH`, and updates the status document.
3. `brightdata_stage` reads the search profiles, writes `{pipeline_id}_BRIGHTDATA`, and reports progress.
4. `llm_fit_stage` consumes the BrightData output, writes `{pipeline_id}_LLM_FIT`, and finalizes progress.
5. The orchestrator calls `complete_pipeline_status` so the status document transitions to `completed`.

Net result: four Firestore documents per pipeline that reflect every step and shared payloads between stages.

## TTL and Cleanup
- `_ttl_value()` in `stage_utils.py` sets `ttl = created_at + 1 hour` on every write.
- Firestore does **not** automatically delete documents based on `ttl`, so the `cleanup_expired_pipelines` scheduled function runs hourly to delete any document where `ttl < now()`.
- The cleanup job queries up to 500 expired documents, groups them by `pipeline_id`, and deletes each pipeline’s documents inside a Firestore batch for atomicity. This keeps storage bounded even during heavy usage.

## Security Rules
- `search_pipeline_runs` can be read only by the authenticated owner (`isSignedIn() && request.auth.uid == resource.data.userId` in `firestore.rules`). Server-rendered routes should proxy data through `adminDb` instead of querying from the browser.
- All client writes are denied so end users cannot forge progress or tamper with results; pipeline writes flow exclusively through Cloud Functions using the Admin SDK, which bypasses security rules.
- The ruleset lives in `firestore.rules` with helper predicates (`isSignedIn`, `isPipelineOwner`) to keep owner checks consistent across status/stage documents.

## Indexes
Composite indexes defined in `firestore.indexes.json`:
1. `pipeline_id ASC, stage ASC` – accelerate `{pipeline_id, stage}` lookups for downstream stages.

Single-field indexes (`pipeline_id`, `stage`, `status`, `ttl`, `created_at`, `updated_at`) are created automatically by Firestore.
The TTL cleanup query (`where('ttl', '<', now).orderBy('ttl')`) relies on these default single-field indexes, so the current `firestore.indexes.json` is up to date.

## Client Integration
- Subscribe to `search_pipeline_runs/{pipeline_id}` using `onSnapshot` to display real-time progress (`status`, `current_stage`, `overall_progress`, `completed_stages`).
- On completion, fetch `{pipeline_id}_LLM_FIT` (or whichever stage you care about) to render the final ranked profiles.
- Remember to unsubscribe once the pipeline finishes to stop paying for reads.

## Monitoring and Debugging
- Use Firebase Console → Firestore → Data to inspect documents and confirm TTL values.
- Log-based monitoring highlights stage-specific errors via `error_message` on either the status or stage documents.
- Useful ad-hoc queries:
  - Running pipelines: `where('status', '==', 'running')`
  - Failed pipelines: `where('status', '==', 'error')`
  - Pipelines stuck in a stage: `where('current_stage', '==', 'BRIGHTDATA').where('updated_at', '<', oneHourAgo)`
  - Count docs per pipeline: `where('pipeline_id', '==', pipelineId).count()` (should equal 5)

## Cost Considerations
- Storage footprint stays minimal (~250 KB per pipeline) because the cleanup job deletes documents after one hour.
- Firestore writes per pipeline: ~20 (pipeline status churn + 4 stage documents). Reads per pipeline: ~4 (each stage reads the previous stage). Deletes per pipeline: 5 (performed by the cleanup job).
- Firestore listeners typically consume < $0.001 per pipeline thanks to the limited lifetime.

## Implementation References
- `functions/stage_utils.py` – helpers for writing/reading documents, TTL handling, status updates.
- `functions/orchestrator.py` – generates `pipeline_id` and coordinates stage execution.
- `functions/search_stage.py`, `functions/rerank_stage.py`, `functions/brightdata_stage.py`, `functions/llm_fit_stage.py` – producers of stage documents.
- `functions/cleanup_expired_pipelines.py` – hourly scheduler that deletes expired documents.

## Verification
- Run `npm run test:firestore:pipeline` to write and read a synthetic pipeline status. With `FIRESTORE_EMULATOR_HOST` set the script targets the local emulator; otherwise it uses your configured project credentials for a production smoke test.
- The script creates `search_pipeline_runs/pipeline_status_test_<uuid>`, validates the payload, and then deletes the document so quotas remain unaffected.
