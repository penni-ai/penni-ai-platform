# Firestore Schema: Search Pipeline

## Overview
The creator search pipeline persists intermediate state and final outputs inside the `search_pipeline_runs` collection. Each pipeline execution writes exactly three documents (one status document plus two stage documents: SEARCH and LIVE_ANALYSIS) so that clients can subscribe to progress, stages can reuse prior results, and operators can debug or replay failures. All documents carry a `ttl` timestamp set to one hour after creation so that the cleanup job can prune them automatically.

## Collection: `search_pipeline_runs`

### Document Types
There are two logical document types stored inside the same collection:

1. **Pipeline status documents** – track overall pipeline progress (document ID: `{pipeline_id}`)
2. **Stage documents** – record detailed data for each stage (document ID: `{pipeline_id}_{STAGE_NAME}`)

---

### 1. Pipeline Status Document

**Document ID:** `{pipeline_id}`

**Purpose:** capture pipeline-wide health, progress, and errors so UI clients have a single place to watch.

**Schema:**
```typescript
{
  pipeline_id: string;                     // UUID generated by the orchestrator
  userId: string;                          // UID of the authenticated caller
  status: "running" | "completed" | "error";   // Current pipeline state
  current_stage: string | null;             // One of SEARCH, LIVE_ANALYSIS
  completed_stages: string[];               // Ordered list of finished stages
  overall_progress: number;                // Percentage (0–100)
  start_time: Timestamp;                   // Pipeline start
  end_time: Timestamp | null;              // Completion or error time
  error_message: string | null;            // Populated when status === "error"
  created_at: Timestamp;                   // Server timestamp when first written
  updated_at: Timestamp;                   // Server timestamp on every write
  ttl: Timestamp;                          // Expiration (created_at + 1 hour)
}
```

**Example:**
```json
{
  "pipeline_id": "1699564800a1b2c3d4e5f6",
  "userId": "user_123",
  "status": "running",
  "current_stage": "LIVE_ANALYSIS",
  "completed_stages": ["SEARCH"],
  "overall_progress": 66,
  "start_time": "2024-11-09T10:00:00Z",
  "end_time": null,
  "error_message": null,
  "created_at": "2024-11-09T10:00:00Z",
  "updated_at": "2024-11-09T10:05:00Z",
  "ttl": "2024-11-09T11:00:00Z"
}
```

**Access pattern:**
- Writes happen exclusively via `create_pipeline_status`, `update_pipeline_status`, `complete_pipeline_status`, and `error_pipeline_status` in `stage_utils.py` (server-side Admin SDK).
- Reads come from authenticated clients that poll progress and from Cloud Functions needing orchestration context.

---

### 2. Stage Document

**Document ID:** `{pipeline_id}_{STAGE_NAME}` (e.g., `1699564800a1b2c3d4e5f6_SEARCH`).

**Purpose:** store serialized `CreatorProfile` results, debug payloads, metadata, and errors for each pipeline stage.

**Schema:**
```typescript
{
  pipeline_id: string;                     // Parent pipeline identifier
  userId: string;                          // UID inherited from the orchestrator request
  stage: "SEARCH" | "LIVE_ANALYSIS";
  status: "completed" | "error";
  profiles: CreatorProfile[];              // Serialized dataclass objects (56+ fields)
  debug: Record<string, any>;              // Stage-specific debug details
  metadata: Record<string, any>;           // IO metadata, stats, durations, etc.
  error_message: string | null;            // Present when status === "error"
  artifacts?: {
    profiles?: StageArtifactRef;           // Pointer to Cloud Storage object with full profile payload
    debug?: StageArtifactRef;              // Pointer to verbose debug payloads
  };
  profiles_snapshot?: Array<{
    account?: string;
    username?: string;
    profile_url?: string;
    fit_score?: number | null;
    combined_score?: number | null;
    lance_db_id?: string | null;
  }>;                                       // Top few profiles kept inline for quick inspection
  created_at: Timestamp;
  updated_at: Timestamp;
  ttl: Timestamp;                          // Expiration (created_at + 1 hour)
}

type StageArtifactRef = {
  bucket: string;
  name: string;                            // Object path: pipelines/{pipeline_id}/{stage}/artifact.json
  size_bytes: number;
  updated: string | null;
};
```

**Example:**
```json
{
  "pipeline_id": "1699564800a1b2c3d4e5f6",
  "userId": "user_123",
  "stage": "SEARCH",
  "status": "completed",
  "profiles": [
    {
      "lance_db_id": "abc123",
      "account": "beauty_influencer",
      "profile_url": "https://instagram.com/beauty_influencer",
      "followers": 50000,
      "engagement_rate": 3.5
      // ...additional CreatorProfile fields
    }
  ],
  "debug": {
    "search_method": "hybrid",
    "query": "beauty influencers",
    "result_count": 20
  },
  "metadata": {
    "io": {
      "inputs": [],
      "outputs": [
        {"lance_db_id": "abc123", "account": "beauty_influencer"}
      ],
      "meta": {}
    }
  },
  "error_message": null,
  "created_at": "2024-11-09T10:00:00Z",
  "updated_at": "2024-11-09T10:00:00Z",
  "ttl": "2024-11-09T11:00:00Z"
}
```

**Access pattern:**
- Writes via `save_stage_document` and `record_stage_failure` from Cloud Functions only.
- Reads performed by downstream stages (`read_stage_document`, `read_stage_profiles`) and optionally by authenticated clients to fetch final results.

---

## Stage Execution Flow
1. `search_pipeline_orchestrator` generates a `pipeline_id` and creates the pipeline status document.
2. `search_stage` executes first, persists `{pipeline_id}_SEARCH`, and updates the status document.
3. `_run_enrichment_stages` in the orchestrator executes both BrightData enrichment and LLM scoring:
   - BrightData batches are processed concurrently to enrich profiles with live data.
   - LLM batches are processed concurrently to score profiles for business fit.
   - A single unified stage document `{pipeline_id}_LIVE_ANALYSIS` is saved at the end containing:
     - Final LLM-scored profiles
     - Combined metadata from both BrightData and LLM operations
     - Nested debug payloads under `brightdata` and `llm` keys
     - Timing information for both operations
4. The orchestrator calls `complete_pipeline_status` so the status document transitions to `completed`.

Net result: three Firestore documents per pipeline (one status + two stage documents) that reflect every step and shared payloads between stages.

### Artifact Offloading

Large payloads (entire profile lists, BrightData/LLM debug blobs) are stored in Cloud Storage under
`pipelines/{pipeline_id}/{stage}/{artifact}.json`. Stage documents keep lightweight pointers in the
`artifacts` map plus a `profiles_snapshot` array for quick inspection. Clients that need the full
data fetch the referenced object (using the Storage emulator at `127.0.0.1:9199` in local dev). This
keeps every Firestore document well below the 1 MB limit without losing fidelity.

## LIVE_ANALYSIS Stage Document Structure

The `LIVE_ANALYSIS` stage document combines results from both BrightData enrichment and LLM scoring operations. This unified structure provides complete information about the enrichment process in a single document.
The full profile list is offloaded to Storage (`artifacts.profiles`); only a snapshot remains inline.

**Metadata Structure:**
```typescript
{
  // Primary output metrics (LLM)
  input_size: { profile_count: number; estimated_bytes: number; ... };
  output_size: { profile_count: number; estimated_bytes: number; ... };
  api_usage: number;
  live_analysis_source: "combined";

  // BrightData metrics (preserved)
  brightdata_flow_metrics: {
    input_count: number;
    deduped_kept: number;
    deduped_discarded: number;
    brightdata_success: number;
    brightdata_dead: number;
    llm_above_5: number;
    llm_below_5: number;
    completed_batches: number;
  };
  brightdata_success_keys: string[];
  brightdata_timing: { duration_ms: number; ... };
  brightdata_input_size: { profile_count: number; ... };
  brightdata_output_size: { profile_count: number; ... };

  // Batch information
  batch_count: number;
  llm_batch_count: number;
}
```

**Debug Structure:**
```typescript
{
  brightdata: {
    brightdata_params: { batch_size: number; concurrency: number; };
    timing: { brightdata_batches_total: {...}; per_batch: {...}; };
    api_stats: { success: number; failed: number; };
    errors: string[];
    success_keys: string[];
    brightdata_results: any[];
    samples: any[];
  };
  llm: {
    profile_fit: any[];
    threshold: number;
    scores: { count: number; avg: number | null; max: number | null; };
    timing: { llm_batches_total: {...}; per_batch: {...}; };
  };
}
```

**Profiles Array:** Contains the final LLM-scored profiles with `fit_score` and `fit_rationale` fields populated.

## Batch Document Structure

Batch documents in the `batches` subcollection track per-batch progress and results:

```typescript
{
  seq: number;
  pipeline_id: string;
  userId: string;

  input_count: number;
  deduped_kept: number;
  deduped_discarded: number;

  brightdata_success: number;
  brightdata_dead: number;
  llm_above_5: number;
  llm_below_5: number;

  status: "live_analysis_completed" | "live_analysis_failed" | "live_analysis_in_progress";

  timing: {
    brightdata_ms: number | null;
    llm_ms: number | null;
    total_ms: number | null;
  };

  profiles: CreatorProfile[];
  profiles_snapshot_count: number;
  profiles_total_in_batch: number;

  errors: string[];

  created_at: Timestamp;
  updated_at: Timestamp;
  ttl: Timestamp;
}
```

**Batch Status Values:**
- `"live_analysis_completed"`: Both BrightData and LLM completed successfully.
- `"live_analysis_failed"`: Either BrightData or LLM failed.
- `"live_analysis_in_progress"`: BrightData completed but LLM hasn't run yet.

## Backward Compatibility

Pipelines created before the LIVE_ANALYSIS stage merge may have separate stage documents:
- `{pipeline_id}_BRIGHTDATA`
- `{pipeline_id}_LLM_FIT`

The `read_stage_document()` function in `stage_utils.py` automatically falls back to these legacy stage names when `LIVE_ANALYSIS` is not found. Legacy documents are marked with a `_legacy_stage_name` field for debugging.

**Migration Strategy:** No migration is required. Old pipelines remain readable through the fallback mechanism, and new pipelines use the unified structure. After the TTL expires (1 hour), old documents are cleaned up automatically.

## TTL and Cleanup
- `_ttl_value()` in `stage_utils.py` sets `ttl = created_at + 1 hour` on every write.
- Firestore does **not** automatically delete documents based on `ttl`, so the `cleanup_expired_pipelines` scheduled function runs hourly to delete any document where `ttl < now()`.
- The cleanup job queries up to 500 expired documents, groups them by `pipeline_id`, and deletes each pipeline’s documents inside a Firestore batch for atomicity. This keeps storage bounded even during heavy usage.

## Security Rules
- `search_pipeline_runs` can be read only by the authenticated owner (`isSignedIn() && request.auth.uid == resource.data.userId` in `firestore.rules`). Server-rendered routes should proxy data through `adminDb` instead of querying from the browser.
- All client writes are denied so end users cannot forge progress or tamper with results; pipeline writes flow exclusively through Cloud Functions using the Admin SDK, which bypasses security rules.
- The ruleset lives in `firestore.rules` with helper predicates (`isSignedIn`, `isPipelineOwner`) to keep owner checks consistent across status/stage documents.

## Indexes
Composite indexes defined in `firestore.indexes.json`:
1. `pipeline_id ASC, stage ASC` – accelerate `{pipeline_id, stage}` lookups for downstream stages.

Single-field indexes (`pipeline_id`, `stage`, `status`, `ttl`, `created_at`, `updated_at`) are created automatically by Firestore.
The TTL cleanup query (`where('ttl', '<', now).orderBy('ttl')`) relies on these default single-field indexes, so the current `firestore.indexes.json` is up to date.

## Client Integration
- Subscribe to `search_pipeline_runs/{pipeline_id}` using `onSnapshot` to display real-time progress (`status`, `current_stage`, `overall_progress`, `completed_stages`).
- On completion, fetch `{pipeline_id}_LIVE_ANALYSIS` (or whichever stage you care about) to render the final ranked profiles.
- Remember to unsubscribe once the pipeline finishes to stop paying for reads.

## Monitoring and Debugging
- Use Firebase Console → Firestore → Data to inspect documents and confirm TTL values.
- Log-based monitoring highlights stage-specific errors via `error_message` on either the status or stage documents.
- Useful ad-hoc queries:
  - Running pipelines: `where('status', '==', 'running')`
  - Failed pipelines: `where('status', '==', 'error')`
  - Pipelines stuck in a stage: `where('current_stage', '==', 'LIVE_ANALYSIS').where('updated_at', '<', oneHourAgo)`
  - Count docs per pipeline: `where('pipeline_id', '==', pipelineId).count()` (should equal 3)

## Cost Considerations
- Storage footprint stays minimal (~250 KB per pipeline) because the cleanup job deletes documents after one hour.
- Firestore writes per pipeline: ~15 (pipeline status churn + 2 stage documents + batch documents). Reads per pipeline: ~4 (each stage reads the previous stage). Deletes per pipeline: 3 + N batches (performed by the cleanup job).
- Firestore listeners typically consume < $0.001 per pipeline thanks to the limited lifetime.

## Implementation References
- `functions/stage_utils.py` – helpers for writing/reading documents, TTL handling, status updates.
- `functions/orchestrator.py` – generates `pipeline_id`, coordinates stages, and persists LIVE_ANALYSIS output.
- `functions/search_stage.py` – produces SEARCH stage documents.
- `functions/brightdata_stage.py`, `functions/llm_fit_stage.py` – batch-level operations invoked by the orchestrator when composing LIVE_ANALYSIS documents.
- `functions/cleanup_expired_pipelines.py` – hourly scheduler that deletes expired documents.

## Verification
- Run `npm run test:firestore:pipeline` to write and read a synthetic pipeline status. With `FIRESTORE_EMULATOR_HOST` set the script targets the local emulator; otherwise it uses your configured project credentials for a production smoke test.
- The script creates `search_pipeline_runs/pipeline_status_test_<uuid>`, validates the payload, and then deletes the document so quotas remain unaffected.
